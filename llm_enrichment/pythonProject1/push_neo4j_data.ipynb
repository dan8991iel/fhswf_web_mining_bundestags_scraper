{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01860f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Konsolidiere 45 Batch-Dateien...\n",
      "📁 Verarbeite batch_000.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 0: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_001.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 1: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_002.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 2: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_003.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 3: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_004.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 4: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_005.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 5: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_006.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 6: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_007.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 7: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_008.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 8: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_009.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 9: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_010.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 10: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_011.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 11: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_012.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 12: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_013.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 13: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_014.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 14: 99 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_015.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 15: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_016.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 16: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_017.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 17: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_018.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 18: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_019.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 19: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_020.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 20: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_021.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 21: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_022.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 22: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_023.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 23: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_024.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 24: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_025.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 25: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_026.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 26: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_027.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 27: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_028.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 28: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_029.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 29: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_030.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 30: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_031.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 31: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_032.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 32: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_033.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 33: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_034.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 34: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_035.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 35: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_036.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 36: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_037.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 37: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_038.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 38: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_039.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 39: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_040.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 40: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_041.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 41: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_042.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 42: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_043.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 43: 100 Ergebnisse hinzugefügt\n",
      "📁 Verarbeite batch_044.json...\n",
      "   Typ: <class 'dict'>\n",
      "   Keys: ['batch_num', 'batch_size', 'successful_processing', 'failed_processing', 'batch_start_time', 'batch_duration', 'total_embedding_time', 'total_llm_time', 'total_llm_costs', 'total_embedding_costs', 'results', 'errors']\n",
      "✅ Batch 44: 55 Ergebnisse hinzugefügt\n",
      "\n",
      " KONSOLIDIERUNG ABGESCHLOSSEN:\n",
      "   Gesamt Ergebnisse: 4454\n",
      "   Erfolgreich: 4454\n",
      "   Fehler: 1\n",
      "   Gesamtkosten: 30.2009€\n",
      "   Gesamtzeit: 6826.9 Sekunden\n",
      "💾 Konsolidierte Daten gespeichert: final_data/neo4j_data_politicians_enriched.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "def consolidate_all_batches():\n",
    "    \"\"\"Konsolidiert alle Batch-Daten in eine gemeinsame Datei\"\"\"\n",
    "    batch_dir = \"final_data/batches\"\n",
    "    output_file = \"final_data/neo4j_data_politicians_enriched.json\"\n",
    "    \n",
    "    # Alle Batch-Dateien laden, aber batch_structure.json ausfiltern\n",
    "    batch_files = [f for f in os.listdir(batch_dir) \n",
    "                   if f.startswith('batch_') and f.endswith('.json') \n",
    "                   and f != 'batch_structure.json']  # NEU: batch_structure ausfiltern\n",
    "    batch_files.sort()\n",
    "    \n",
    "    print(f\"📦 Konsolidiere {len(batch_files)} Batch-Dateien...\")\n",
    "    \n",
    "    \n",
    "    # Aggregierte Metadaten\n",
    "    consolidated_data = {\n",
    "        \"metadata\": {\n",
    "            \"total_batches\": len(batch_files),\n",
    "            \"consolidation_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_successful_processing\": 0,\n",
    "            \"total_failed_processing\": 0,\n",
    "            \"total_embedding_time\": 0,\n",
    "            \"total_llm_time\": 0,\n",
    "            \"total_embedding_costs\": 0,\n",
    "            \"total_llm_costs\": 0,\n",
    "            \"total_batch_duration\": 0\n",
    "        },\n",
    "        \"results\": []\n",
    "    }\n",
    "    \n",
    "    # Durch alle Batches iterieren\n",
    "    for i, batch_file in enumerate(batch_files):\n",
    "        try:\n",
    "            print(f\"📁 Verarbeite {batch_file}...\")\n",
    "            \n",
    "            with open(os.path.join(batch_dir, batch_file), 'r') as f:\n",
    "                batch_data = json.load(f)\n",
    "            \n",
    "            print(f\"   Typ: {type(batch_data)}\")\n",
    "            print(f\"   Keys: {list(batch_data.keys())}\")\n",
    "            \n",
    "            # Metadaten aggregieren\n",
    "            consolidated_data[\"metadata\"][\"total_successful_processing\"] += batch_data[\"successful_processing\"]\n",
    "            consolidated_data[\"metadata\"][\"total_failed_processing\"] += batch_data[\"failed_processing\"]\n",
    "            consolidated_data[\"metadata\"][\"total_embedding_time\"] += batch_data[\"total_embedding_time\"]\n",
    "            consolidated_data[\"metadata\"][\"total_llm_time\"] += batch_data[\"total_llm_time\"]\n",
    "            consolidated_data[\"metadata\"][\"total_embedding_costs\"] += batch_data[\"total_embedding_costs\"]\n",
    "            consolidated_data[\"metadata\"][\"total_llm_costs\"] += batch_data[\"total_llm_costs\"]\n",
    "            consolidated_data[\"metadata\"][\"total_batch_duration\"] += batch_data[\"batch_duration\"]\n",
    "            \n",
    "            # Alle Ergebnisse in die gemeinsame Liste\n",
    "            consolidated_data[\"results\"].extend(batch_data[\"results\"])\n",
    "            \n",
    "            print(f\"✅ Batch {batch_data['batch_num']}: {batch_data['successful_processing']} Ergebnisse hinzugefügt\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Fehler bei {batch_file}: {e}\")\n",
    "            print(f\"   Typ: {type(e).__name__}\")\n",
    "            continue\n",
    "    \n",
    "    # Gesamtkosten berechnen\n",
    "    total_costs = consolidated_data[\"metadata\"][\"total_embedding_costs\"] + consolidated_data[\"metadata\"][\"total_llm_costs\"]\n",
    "    \n",
    "    print(f\"\\n KONSOLIDIERUNG ABGESCHLOSSEN:\")\n",
    "    print(f\"   Gesamt Ergebnisse: {len(consolidated_data['results'])}\")\n",
    "    print(f\"   Erfolgreich: {consolidated_data['metadata']['total_successful_processing']}\")\n",
    "    print(f\"   Fehler: {consolidated_data['metadata']['total_failed_processing']}\")\n",
    "    print(f\"   Gesamtkosten: {total_costs:.4f}€\")\n",
    "    print(f\"   Gesamtzeit: {consolidated_data['metadata']['total_batch_duration']:.1f} Sekunden\")\n",
    "    \n",
    "    # In Datei speichern\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(consolidated_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"💾 Konsolidierte Daten gespeichert: {output_file}\")\n",
    "    \n",
    "    return consolidated_data\n",
    "\n",
    "# Ausführen\n",
    "consolidated_data = consolidate_all_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd97c3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Lade 4454 Klassifikationen...\n",
      "🚀 Starte Neo4j-Import...\n",
      "✅ Import erfolgreich!\n",
      "   Erstellte Nodes: 4454\n",
      "   Dauer: 1.2 Sekunden\n",
      "   Durchschnitt: 0.3 ms pro Klassifikation\n",
      "🔍 VERIFIKATION:\n",
      "   Gesamt Klassifikationen: 4454\n",
      "   Politiker mit Klassifikation: 4454\n",
      "🔒 Neo4j-Verbindung geschlossen\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class Neo4jClassifier:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def push_classifications_to_neo4j(self, classifications_file=\"final_data/neo4j_data_politicians_enriched.json\"):\n",
    "        \"\"\"Fügt alle Klassifikationen in Neo4j ein\"\"\"\n",
    "        \n",
    "        # Klassifikationen laden\n",
    "        with open(classifications_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        classifications = data['results']\n",
    "        print(f\"📊 Lade {len(classifications)} Klassifikationen...\")\n",
    "        \n",
    "        # Daten für Neo4j vorbereiten\n",
    "        neo4j_data = []\n",
    "        for classification in classifications:\n",
    "            neo4j_data.append({\n",
    "                'neo4j_element_id': classification['neo4j_element_id'],\n",
    "                'dqr_level': classification['dqr_predict'],\n",
    "                'comment': classification['comment_predict'],\n",
    "                'confidence': classification['confidence_score']\n",
    "            })\n",
    "        \n",
    "        print(f\"🚀 Starte Neo4j-Import...\")\n",
    "        \n",
    "        # Batch-INSERT Query\n",
    "        query = \"\"\"\n",
    "        UNWIND $classifications AS classification\n",
    "        MATCH (p:Politician)\n",
    "        WHERE elementId(p) = classification.neo4j_element_id\n",
    "        CREATE (c:Classification {\n",
    "            dqr_level: classification.dqr_level,\n",
    "            comment: classification.comment,\n",
    "            confidence: classification.confidence\n",
    "        })\n",
    "        CREATE (p)-[:HAS_CLASSIFICATION]->(c)\n",
    "        RETURN count(c) as created_nodes\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                start_time = datetime.now()\n",
    "                \n",
    "                result = session.run(query, classifications=neo4j_data)\n",
    "                created_count = result.single()['created_nodes']\n",
    "                \n",
    "                duration = (datetime.now() - start_time).total_seconds()\n",
    "                \n",
    "                print(f\"✅ Import erfolgreich!\")\n",
    "                print(f\"   Erstellte Nodes: {created_count}\")\n",
    "                print(f\"   Dauer: {duration:.1f} Sekunden\")\n",
    "                print(f\"   Durchschnitt: {duration/len(classifications)*1000:.1f} ms pro Klassifikation\")\n",
    "                \n",
    "                return created_count\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Fehler beim Import: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def verify_import(self):\n",
    "        \"\"\"Überprüft ob der Import erfolgreich war\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Politician)-[:HAS_CLASSIFICATION]->(c:Classification)\n",
    "        RETURN count(c) as total_classifications,\n",
    "               count(DISTINCT p) as politicians_with_classification\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(query)\n",
    "                stats = result.single()\n",
    "                \n",
    "                print(f\"🔍 VERIFIKATION:\")\n",
    "                print(f\"   Gesamt Klassifikationen: {stats['total_classifications']}\")\n",
    "                print(f\"   Politiker mit Klassifikation: {stats['politicians_with_classification']}\")\n",
    "                \n",
    "                return stats\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Fehler bei Verifikation: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Neo4j-Verbindung\n",
    "classifier = Neo4jClassifier(\n",
    "    uri=\"bolt://localhost:7687\",\n",
    "    user=\"neo4j\", \n",
    "    password=\"bundestag_password\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Klassifikationen einfügen\n",
    "    created_count = classifier.push_classifications_to_neo4j()\n",
    "    \n",
    "    # Verifikation\n",
    "    classifier.verify_import()\n",
    "    \n",
    "finally:\n",
    "    classifier.close()\n",
    "    print(\"🔒 Neo4j-Verbindung geschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197636ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
